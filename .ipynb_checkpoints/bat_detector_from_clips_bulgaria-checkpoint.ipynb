{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b25483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "import scipy.ndimage.morphology as morph\n",
    "from scipy.ndimage.filters import median_filter\n",
    "import scipy.ndimage\n",
    "from skimage.measure import regionprops\n",
    "import pandas as pd\n",
    "\n",
    "#import bat_train.evaluate as evl\n",
    "from scipy.io import wavfile\n",
    "#from helper_fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31159c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set: bulgaria\n"
     ]
    }
   ],
   "source": [
    "test_set      = 'bulgaria'  # can be one of: bulgaria, uk, norfolk\n",
    "data_set      = 'bat_train/data/train_test_split/test_set_' + test_set + '.npz'\n",
    "raw_audio_dir = 'bat_train/data/wav/'\n",
    "clip_audio_dir= 'bat_train/data/wav_clips/'\n",
    "base_line_dir = 'bat_train/data/baselines/'\n",
    "result_dir    = 'bat_train/results/'\n",
    "model_dir     = 'bat_train/data/models/'\n",
    "if not os.path.isdir(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "print('test set:', test_set)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecdbafe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['not_bat', 'bat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e442cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(clip_audio_dir+test_set)\n",
    "non_bat_dir = clip_audio_dir+test_set+'/'+classes[0]+'/'\n",
    "bat_dir     = clip_audio_dir+test_set+'/'+classes[1]+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5642bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "bat_paths     = [bat_dir + fn for fn in os.listdir(bat_dir)]\n",
    "non_bat_paths = [non_bat_dir + fn for fn in os.listdir(non_bat_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "321a63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaa400db",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(bat_paths)\n",
    "shuffle(non_bat_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af384943",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = 0.8\n",
    "bat_cutoff = int(train_perc*len(bat_paths))\n",
    "non_bat_cutoff = int(train_perc*len(non_bat_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6b828b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = bat_paths[:bat_cutoff]\n",
    "train_paths.extend(non_bat_paths[:non_bat_cutoff])\n",
    "test_paths = bat_paths[bat_cutoff:]\n",
    "test_paths.extend(non_bat_paths[non_bat_cutoff:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6404e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(train_paths)\n",
    "shuffle(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1eda3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: define functions that get the different channels, e.g. regular spectrogram, real, im, arg, harmonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcbd6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bat_train.data_set_params import DataSetParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b23ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = DataSetParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3972e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_complex_spec(audio_samp, samp_rate, params):\n",
    "    nfft     = int(params.fft_win_length*samp_rate)\n",
    "    noverlap = int(params.fft_overlap*nfft)\n",
    "\n",
    "    # window data\n",
    "    step    = nfft - noverlap\n",
    "    #print(step)\n",
    "    shape   = (nfft, (audio_samp.shape[-1]-noverlap)//step)\n",
    "    strides = (audio_samp.strides[0], step*audio_samp.strides[0])\n",
    "    x_wins  = np.lib.stride_tricks.as_strided(audio_samp, shape=shape, strides=strides)\n",
    "    x_wins_han = np.hanning(x_wins.shape[0])[..., np.newaxis] * x_wins\n",
    "    \n",
    "    # do fft\n",
    "    # note this will be much slower if x_wins_han.shape[0] is not a power of 2\n",
    "    complex_spec = np.fft.rfft(x_wins_han, axis=0)\n",
    "    return complex_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea2aea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_spec_stack(complex_spec, choices = ['Mod']):\n",
    "    #possible components:\n",
    "    ##Mod    modulus/absolute value (regular)\n",
    "    ##Re     real component\n",
    "    ##AbsRe  abs(real component)\n",
    "    ##Im     imaginary component\n",
    "    ##AbsIm  abs(imaginary component)\n",
    "    ##Ang    angular componant\n",
    "    ##AbsAng abs(angular componant)\n",
    "    spec_arr = []\n",
    "    for c in choices:\n",
    "        def apply_func(x,c):\n",
    "            if c == 'Mod':\n",
    "                return tf.math.abs(x)\n",
    "            if c == 'Re':\n",
    "                return tf.math.real(x)\n",
    "            if c == 'AbsRe':\n",
    "                return tf.math.abs(tf.math.real(x))\n",
    "            if c == 'Im':\n",
    "                return tf.math.imag(x)\n",
    "            if c == 'AbsIm':\n",
    "                return tf.math.abs(tf.math.imag(x))\n",
    "            if c == 'Ang':\n",
    "                return tf.math.ang(x)\n",
    "            if c == 'AbsAng':\n",
    "                return tf.math.abs(tf.math.imag(x))\n",
    "        spec_arr.append(apply_func(complex_spec,c))\n",
    "    return np.stack(spec_arr,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9358dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_spec_post(spec, samp_rate):\n",
    "    spec = spec[1:, :, :]\n",
    "    spec = np.flipud(spec)\n",
    "    # only keep the relevant bands - could do this outside\n",
    "    if params.crop_spec:\n",
    "        spec = spec[-params.max_freq:-params.min_freq, :, :]\n",
    "    \n",
    "        # add some zeros if too small\n",
    "        req_height = params.max_freq-params.min_freq\n",
    "        if spec.shape[0] < req_height:\n",
    "            zero_pad = np.zeros((req_height-spec.shape[0], spec.shape[1], spec.shape[2]))\n",
    "            spec     = np.vstack((zero_pad, spec))\n",
    "\n",
    "    # perform log scaling - here the same as matplotlib\n",
    "    log_scaling = 2.0 * (1.0 / samp_rate) * (1.0/(np.abs(np.hanning(int(params.fft_win_length*samp_rate)))**2).sum())\n",
    "    spec        = np.log(1.0 + log_scaling*spec)\n",
    "    def spec_normalize(x, axis = (0,1), kd = True):\n",
    "        return (x- x.min(axis=axis, keepdims=kd))/(x.max(axis=axis, keepdims=kd) - x.min(axis=axis, keepdims=kd))\n",
    "    spec = spec_normalize(spec)\n",
    "    return(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d31f4bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_shape = (260, 36, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "602c1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multichannel_features(paths, params, choices = ['Mod']):\n",
    "    feats = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    ndiv = len(paths) // 10\n",
    "    spec_shape = (260, 36, len(choices))\n",
    "    for file_name in paths:\n",
    "        #print(file_name)\n",
    "        if count % ndiv == 0:\n",
    "            print(count+1, '/', len(paths))\n",
    "        count = count + 1\n",
    "        samp_rate, audio_samp = wavfile.read(file_name)\n",
    "        #print('samp_rate:',samp_rate)\n",
    "        #print('len(audio_samp):', len(audio_samp))\n",
    "        if len(audio_samp)/samp_rate < 0.1:\n",
    "            #os.remove(file_name)\n",
    "            continue \n",
    "        labels.append(file_name.split('/')[-2])\n",
    "        complex_spec = gen_complex_spec(audio_samp, samp_rate, params)\n",
    "        multi_spec = multi_spec_stack(complex_spec, choices)\n",
    "        multi_spec = multi_spec_post(multi_spec, samp_rate)\n",
    "        if multi_spec.shape[1] < spec_shape[1]:\n",
    "            #print('1',multi_spec.shape)\n",
    "            zero_pad = np.zeros((spec_shape[0], spec_shape[1] - multi_spec.shape[1], spec_shape[2]))\n",
    "            multi_spec = np.hstack((multi_spec, zero_pad))\n",
    "            #print('2',multi_spec.shape)\n",
    "        feats.append(multi_spec)\n",
    "    #print([fts.shape for fts in feats])\n",
    "    features = np.stack(feats, axis=0)\n",
    "    #features = np.squeeze(features)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa057830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30626\n",
      "3063 / 30626\n",
      "6125 / 30626\n",
      "9187 / 30626\n",
      "12249 / 30626\n",
      "15311 / 30626\n",
      "18373 / 30626\n",
      "21435 / 30626\n",
      "24497 / 30626\n",
      "27559 / 30626\n",
      "30621 / 30626\n",
      "1 / 7657\n",
      "766 / 7657\n",
      "1531 / 7657\n",
      "2296 / 7657\n",
      "3061 / 7657\n",
      "3826 / 7657\n",
      "4591 / 7657\n",
      "5356 / 7657\n",
      "6121 / 7657\n",
      "6886 / 7657\n",
      "7651 / 7657\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_multichannel_features(train_paths, params)\n",
    "test_features,  test_labels  = get_multichannel_features(test_paths, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1703b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize = (18,5))\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(x = train_labels, ax=axes[0])\n",
    "lb = axes[0].get_xlabel()\n",
    "axes[0].set_xlabel(lb, fontsize=15)\n",
    "axes[0].set_title(\"Train\", size = 14)\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.countplot(x = test_labels, ax=axes[1])\n",
    "lbt = axes[1].get_xlabel()\n",
    "axes[1].set_xlabel(lbt, fontsize=15)\n",
    "axes[1].set_title(\"Test\", size = 14)\n",
    "plt.suptitle(test_set.capitalize())\n",
    "\n",
    "for ax in axes:\n",
    "    for p in ax.patches:\n",
    "        #ax.annotate('{}'.format(p.get_height()), \n",
    "        #            ((p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center',))\n",
    "        ax.annotate(format(p.get_height(), '.1f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c810c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_bin = [classes.index(lab) for lab in train_labels]\n",
    "test_labels_bin  = [classes.index(lab) for lab in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd90c2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30617"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "801b9690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30617, 260, 36, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3957b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds   = tf.data.Dataset.from_tensor_slices((train_features, train_labels_bin))\n",
    "test_ds    = tf.data.Dataset.from_tensor_slices((test_features, test_labels_bin))\n",
    "batch_size = 128\n",
    "train_ds   = train_ds.batch(batch_size)\n",
    "test_ds    = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baf7c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_features.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "553cd37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 258, 34, 16)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 129, 17, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 127, 15, 16)       2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 63, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 7056)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 7056)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                451648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 454,258\n",
      "Trainable params: 454,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "small_cnn = models.Sequential([\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Conv2D(16, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(16, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(2, activation = 'softmax'),\n",
    "    ])\n",
    "small_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3124c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss      = tf.keras.losses.SparseCategoricalCrossentropy(),#from_logits=True),\n",
    "    metrics   = 'accuracy',#tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d910bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "240/240 [==============================] - 91s 376ms/step - loss: 0.6851 - accuracy: 0.6658 - val_loss: 0.5926 - val_accuracy: 0.7422\n",
      "Epoch 2/25\n",
      "240/240 [==============================] - 94s 392ms/step - loss: 0.5654 - accuracy: 0.7509 - val_loss: 0.5095 - val_accuracy: 0.8056\n",
      "Epoch 3/25\n",
      "240/240 [==============================] - 64s 269ms/step - loss: 0.5011 - accuracy: 0.7933 - val_loss: 0.4578 - val_accuracy: 0.8308\n",
      "Epoch 4/25\n",
      "240/240 [==============================] - 92s 385ms/step - loss: 0.4594 - accuracy: 0.8176 - val_loss: 0.4297 - val_accuracy: 0.8458\n",
      "Epoch 5/25\n",
      "240/240 [==============================] - 97s 404ms/step - loss: 0.4376 - accuracy: 0.8285 - val_loss: 0.4132 - val_accuracy: 0.8547\n",
      "Epoch 6/25\n",
      "240/240 [==============================] - 99s 413ms/step - loss: 0.4193 - accuracy: 0.8376 - val_loss: 0.3976 - val_accuracy: 0.8580\n",
      "Epoch 7/25\n",
      "240/240 [==============================] - 67s 279ms/step - loss: 0.4088 - accuracy: 0.8418 - val_loss: 0.3870 - val_accuracy: 0.8609\n",
      "Epoch 8/25\n",
      "240/240 [==============================] - 82s 343ms/step - loss: 0.4001 - accuracy: 0.8453 - val_loss: 0.3789 - val_accuracy: 0.8648\n",
      "Epoch 9/25\n",
      "240/240 [==============================] - 93s 386ms/step - loss: 0.3918 - accuracy: 0.8486 - val_loss: 0.3746 - val_accuracy: 0.8675\n",
      "Epoch 10/25\n",
      "240/240 [==============================] - 93s 387ms/step - loss: 0.3829 - accuracy: 0.8524 - val_loss: 0.3659 - val_accuracy: 0.8687\n",
      "Epoch 11/25\n",
      "240/240 [==============================] - 94s 390ms/step - loss: 0.3763 - accuracy: 0.8561 - val_loss: 0.3593 - val_accuracy: 0.8690\n",
      "Epoch 12/25\n",
      "240/240 [==============================] - 93s 389ms/step - loss: 0.3710 - accuracy: 0.8573 - val_loss: 0.3555 - val_accuracy: 0.8734\n",
      "Epoch 13/25\n",
      "240/240 [==============================] - 95s 395ms/step - loss: 0.3676 - accuracy: 0.8590 - val_loss: 0.3503 - val_accuracy: 0.8755\n",
      "Epoch 14/25\n",
      "240/240 [==============================] - 92s 384ms/step - loss: 0.3615 - accuracy: 0.8615 - val_loss: 0.3449 - val_accuracy: 0.8763\n",
      "Epoch 15/25\n",
      "240/240 [==============================] - 70s 290ms/step - loss: 0.3569 - accuracy: 0.8649 - val_loss: 0.3410 - val_accuracy: 0.8784\n",
      "Epoch 16/25\n",
      "240/240 [==============================] - 85s 353ms/step - loss: 0.3513 - accuracy: 0.8666 - val_loss: 0.3373 - val_accuracy: 0.8802\n",
      "Epoch 17/25\n",
      "240/240 [==============================] - 64s 266ms/step - loss: 0.3474 - accuracy: 0.8673 - val_loss: 0.3333 - val_accuracy: 0.8807\n",
      "Epoch 18/25\n",
      "240/240 [==============================] - 93s 388ms/step - loss: 0.3444 - accuracy: 0.8706 - val_loss: 0.3294 - val_accuracy: 0.8824\n",
      "Epoch 19/25\n",
      "240/240 [==============================] - 107s 445ms/step - loss: 0.3383 - accuracy: 0.8726 - val_loss: 0.3257 - val_accuracy: 0.8853\n",
      "Epoch 20/25\n",
      "240/240 [==============================] - 104s 434ms/step - loss: 0.3370 - accuracy: 0.8724 - val_loss: 0.3221 - val_accuracy: 0.8833\n",
      "Epoch 21/25\n",
      "240/240 [==============================] - 108s 451ms/step - loss: 0.3303 - accuracy: 0.8754 - val_loss: 0.3180 - val_accuracy: 0.8852\n",
      "Epoch 22/25\n",
      "240/240 [==============================] - 105s 436ms/step - loss: 0.3299 - accuracy: 0.8754 - val_loss: 0.3156 - val_accuracy: 0.8883\n",
      "Epoch 23/25\n",
      "240/240 [==============================] - 104s 433ms/step - loss: 0.3294 - accuracy: 0.8748 - val_loss: 0.3143 - val_accuracy: 0.8904\n",
      "Epoch 24/25\n",
      "240/240 [==============================] - 106s 441ms/step - loss: 0.3245 - accuracy: 0.8779 - val_loss: 0.3111 - val_accuracy: 0.8893\n",
      "Epoch 25/25\n",
      "235/240 [============================>.] - ETA: 1s - loss: 0.3210 - accuracy: 0.8801"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-9b53b2031d10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m history = small_cnn.fit(train_ds, \n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS  = 25\n",
    "history = small_cnn.fit(train_ds, \n",
    "    validation_data = test_ds,  \n",
    "    epochs          = EPOCHS,\n",
    "    callbacks       = tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.epoch, metrics['accuracy'], metrics['val_accuracy'])\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f460572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\").replace('-', '_')+'_'+test_set+'_shallow_net.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to chop off top of cnn to work with different number of input channels\n",
    "choices = ['AbsRe', 'AbsIm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_mult, train_labels_mult = get_multichannel_features(train_paths, params, choices=choices)\n",
    "test_features_mult,  test_labels_mult  = get_multichannel_features(test_paths,  params, choices=choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_mult_bin = [classes.index(lab) for lab in train_labels_mult]\n",
    "test_labels_mult_bin  = [classes.index(lab) for lab in test_labels_mult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_mult   = tf.data.Dataset.from_tensor_slices((train_features_mult, train_labels_mult_bin))\n",
    "test_ds_mult    = tf.data.Dataset.from_tensor_slices((test_features_mult, test_labels_mult_bin))\n",
    "batch_size      = 128\n",
    "train_ds_mult   = train_ds_mult.batch(batch_size)\n",
    "test_ds_mult    = test_ds_mult.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2696e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_config = small_cnn.get_config()\n",
    "tf_shape = train_features_mult.shape[1:]\n",
    "small_cnn_config[\"layers\"][0][\"config\"][\"batch_input_shape\"] = (None, tf_shape[0], tf_shape[1], tf_shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd8271",
   "metadata": {},
   "source": [
    "This model will be the trained with the parameters from the small_cnn model, but the weights in the first conv layer are copied multiple times to match channels/dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult = tf.keras.Sequential.from_config(small_cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1389ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to copy the weights from other model\n",
    "#from https://towardsdatascience.com/implementing-transfer-learning-from-rgb-to-multi-channel-imagery-f87924679166\n",
    "# Expand weights dimension to match new input channels\n",
    "def multify_weights(kernel, out_channels):\n",
    "    mean_1d = np.mean(kernel, axis=-2).reshape(kernel[:,:,-1:,:].shape)\n",
    "    tiled   = np.tile(mean_1d, (out_channels, 1))\n",
    "    return(tiled)\n",
    "\n",
    "\n",
    "# Loop through layers of both original model \n",
    "# and custom model and copy over weights \n",
    "# layer_modify refers to first convolutional layer\n",
    "def copy_weights_tl(model_orig, custom_model, layer_modify):\n",
    "    layer_to_modify = [layer_modify]\n",
    "\n",
    "    conf = custom_model.get_config()\n",
    "    layer_names   = [conf['layers'][x]['config']['name'] for x in range(len(conf['layers']))]\n",
    "    input_channel = conf[\"layers\"][0][\"config\"][\"batch_input_shape\"][-1]\n",
    "    #old_input_channel = model_orig.get_config()[\"layers\"][0][\"config\"][\"batch_input_shape\"][-1]\n",
    "\n",
    "    for layer in model_orig.layers:\n",
    "        if layer.name in layer_names:\n",
    "            if layer.get_weights() != []:\n",
    "                target_layer = custom_model.get_layer(layer.name)\n",
    "\n",
    "                if layer.name in layer_to_modify:    \n",
    "                    kernels = layer.get_weights()[0]\n",
    "                    biases  = layer.get_weights()[1]\n",
    "            \n",
    "                    kernels_extra_channel = multify_weights(kernels, input_channel)\n",
    "                    #print('kernels_extra_channel', kernels_extra_channel.shape)                                \n",
    "                    target_layer.set_weights([kernels_extra_channel, biases])\n",
    "                    target_layer.trainable = False\n",
    "\n",
    "                else:\n",
    "                    target_layer.set_weights(layer.get_weights())\n",
    "                    target_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12925de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_weights_tl(small_cnn, small_cnn_mult, small_cnn_config[\"layers\"][1]['config']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eedd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss      = tf.keras.losses.SparseCategoricalCrossentropy(),#from_logits=True),\n",
    "    metrics   = 'accuracy',#tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdda10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult.evaluate(test_ds_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fa4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS  = 5\n",
    "history = small_cnn_mult.fit(train_ds_mult, \n",
    "    validation_data = test_ds_mult,  \n",
    "    epochs          = EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\").replace('-', '_')+'_'+test_set+'_shallow_net_'+''.join(choices)+'.h5'\n",
    "small_cnn_mult.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e12ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult_fresh = tf.keras.Sequential.from_config(small_cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult_fresh.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss      = tf.keras.losses.SparseCategoricalCrossentropy(),#from_logits=True),\n",
    "    metrics   = 'accuracy',#tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c34efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult_fresh.evaluate(test_ds_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d55323",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS  = 25\n",
    "history = small_cnn_mult_fresh.fit(train_ds_mult, \n",
    "    validation_data = test_ds_mult,  \n",
    "    epochs          = EPOCHS,\n",
    "    callbacks       = tf.keras.callbacks.EarlyStopping(verbose=1, patience=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\").replace('-', '_')+'_'+test_set+'_shallow_net_'+''.join(choices)+'_fresh.h5'\n",
    "small_cnn_mult.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fd56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained on multiple channels, evaluate on one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb16310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult_config = small_cnn_mult.get_config()\n",
    "tf_shape = train_features.shape[1:]\n",
    "small_cnn_mult_config[\"layers\"][0][\"config\"][\"batch_input_shape\"] = (None, tf_shape[0], tf_shape[1], tf_shape[2])\n",
    "small_cnn_mult_single = tf.keras.Sequential.from_config(small_cnn_mult_config)\n",
    "copy_weights_tl(small_cnn_mult, small_cnn_mult_single, small_cnn_mult_config[\"layers\"][1]['config']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult_single.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss      = tf.keras.losses.SparseCategoricalCrossentropy(),#from_logits=True),\n",
    "    metrics   = 'accuracy',#tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn_mult_single.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS  = 5\n",
    "history = small_cnn_mult_single.fit(train_ds, \n",
    "    validation_data = test_ds,  \n",
    "    epochs          = EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471946f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\").replace('-', '_')+'_'+test_set+'_shallow_net_'+''.join(choices)+'_mult_single.h5'\n",
    "small_cnn_mult.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ad4b4",
   "metadata": {},
   "source": [
    "\n",
    "Test the following:\n",
    "- multichannel evaluated on pre-trained single channel model\n",
    "- multichannel trained for a small number of epochs on  pre-trained single channel model\n",
    "- multichannel trained from scratch\n",
    "\n",
    "Test the following:\n",
    "    \n",
    "- single channel evaluated on pre-trained multichannel model \n",
    "- single channel trained for a small number of epochs on  pre-trained multichannel model \n",
    "- single channel trained from scratch "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
